# app.py
import streamlit as st
from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
import os # <--- ADDED THIS IMPORT

# --- à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ ---
st.set_page_config(page_title="VSD Virtual Mentor", layout="wide")
st.title("ðŸ‘¨â€ðŸ”§ VSD Maintenance: Virtual Mentor")
st.markdown("à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸§à¸´à¸¨à¸§à¸à¸£à¹€à¸ªà¸¡à¸·à¸­à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸•à¸­à¸šà¸›à¸±à¸à¸«à¸²à¸à¸²à¸£à¸šà¸³à¸£à¸¸à¸‡à¸£à¸±à¸à¸©à¸² Variable Speed Drive")

# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² API Key à¸‚à¸­à¸‡ Google
# à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ st.secrets à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸ˆà¸£à¸´à¸‡
# For testing, you can paste your key directly
# GOOGLE_API_KEY = "AIzaSyDM0N01ki1RbULaMPK0aObuTNLl9weVyqU"
GOOGLE_API_KEY = "" # Initialize to avoid NameError
try:
    from google_api_key import GOOGLE_API_KEY
except ImportError:
    GOOGLE_API_KEY = st.text_input("à¸à¸£à¸¸à¸“à¸²à¹ƒà¸ªà¹ˆ Google API Key à¸‚à¸­à¸‡à¸„à¸¸à¸“:", type="password")

if not GOOGLE_API_KEY:
    st.warning("à¸à¸£à¸¸à¸“à¸²à¹ƒà¸ªà¹ˆ Google API Key à¹€à¸žà¸·à¹ˆà¸­à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
    st.stop()
    
# --- à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸à¸‚à¸­à¸‡ RAG ---
@st.cache_resource # Cache resource à¹€à¸žà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹ƒà¸«à¸¡à¹ˆà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡
def load_rag_pipeline():
    # 1. à¹‚à¸«à¸¥à¸” Embedding model (à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸•à¸­à¸™ ingest)
    model_name = "intfloat/multilingual-e5-large"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    # 2. à¹‚à¸«à¸¥à¸” Vector Database à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¹„à¸§à¹‰
    vectordb = Chroma(persist_directory="./vs_db", embedding_function=embeddings)

    # 3. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² LLM (Gemini Pro)
    llm = GoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)

    # 4. à¸ªà¸£à¹‰à¸²à¸‡ Prompt Template à¹€à¸žà¸·à¹ˆà¸­ "à¸ªà¸­à¸™" AI à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸§à¸´à¸¨à¸§à¸à¸£
    prompt_template = """
    à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¸šà¸³à¸£à¸¸à¸‡à¸£à¸±à¸à¸©à¸² VSD (Variable Speed Drive) à¸£à¸°à¸”à¸±à¸šà¸‹à¸µà¹€à¸™à¸µà¸¢à¸£à¹Œ
    à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¸‚à¸­à¸‡à¸„à¸¸à¸“à¸„à¸·à¸­à¹ƒà¸«à¹‰à¸„à¸³à¹à¸™à¸°à¸™à¸³à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ à¹à¸¡à¹ˆà¸™à¸¢à¸³ à¹à¸¥à¸°à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¹à¸à¹ˆà¸§à¸´à¸¨à¸§à¸à¸£à¸«à¸£à¸·à¸­à¸Šà¹ˆà¸²à¸‡à¹€à¸—à¸„à¸™à¸´à¸„
    à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸šà¸£à¸´à¸šà¸—à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸¡à¸²à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡à¸™à¸µà¹‰à¹€à¸žà¸·à¹ˆà¸­à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸­à¸¢à¹ˆà¸²à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¸‚à¸¶à¹‰à¸™à¸¡à¸²à¹€à¸­à¸‡

    **à¸ªà¸³à¸„à¸±à¸:**
    - à¸«à¸²à¸à¸¡à¸µà¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸šà¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢ (à¹€à¸Šà¹ˆà¸™ à¸à¸²à¸£à¸•à¸±à¸”à¹„à¸Ÿ, Lockout-Tagout) à¹ƒà¸«à¹‰à¹€à¸™à¹‰à¸™à¸¢à¹‰à¸³à¹€à¸ªà¸¡à¸­
    - à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™ (step-by-step) à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™à¹„à¸›à¹„à¸”à¹‰
    - à¸–à¹‰à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¸šà¸£à¸´à¸šà¸—à¹„à¸¡à¹ˆà¹€à¸žà¸µà¸¢à¸‡à¸žà¸­à¸—à¸µà¹ˆà¸ˆà¸°à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡ à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸§à¹ˆà¸² "à¸œà¸¡à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹ƒà¸™à¸à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹‚à¸›à¸£à¸”à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ˆà¸²à¸à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹‚à¸”à¸¢à¸•à¸£à¸‡"

    **à¸šà¸£à¸´à¸šà¸—:**
    {context}

    **à¸„à¸³à¸–à¸²à¸¡:**
    {question}

    **à¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ:**
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    # 5. à¸ªà¸£à¹‰à¸²à¸‡ RetrievalQA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectordb.as_retriever(search_kwargs={'k': 3}), # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸¡à¸² 3 à¸Šà¸´à¹‰à¸™
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# --- à¸ªà¹ˆà¸§à¸™à¸‚à¸­à¸‡à¸«à¸™à¹‰à¸²à¸ˆà¸­à¹à¸­à¸›à¸žà¸¥à¸´à¹€à¸„à¸Šà¸±à¸™ ---
rag_pipeline = load_rag_pipeline()

if "messages" not in st.session_state:
    st.session_state.messages = []

# à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹à¸Šà¸—à¹€à¸à¹ˆà¸²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# à¸£à¸±à¸š Input à¸ˆà¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰
if prompt := st.chat_input("à¸–à¸²à¸¡à¸›à¸±à¸à¸«à¸²à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š VSD à¹„à¸”à¹‰à¹€à¸¥à¸¢..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    full_response = "" # Initialize full_response before the try block
    with st.chat_message("assistant"):
        # --- DEBUGGING STARTS HERE ---
        st.write("1. à¹„à¸”à¹‰à¸£à¸±à¸šà¸„à¸³à¸–à¸²à¸¡à¹à¸¥à¹‰à¸§ à¸à¸³à¸¥à¸±à¸‡à¸ˆà¸°à¹€à¸£à¸µà¸¢à¸ RAG pipeline...")
        
        try:
            with st.spinner("à¸à¸³à¸¥à¸±à¸‡à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸°à¹€à¸£à¸µà¸¢à¸šà¹€à¸£à¸µà¸¢à¸‡à¸„à¸³à¸•à¸­à¸š..."):
                st.write("2. à¹€à¸‚à¹‰à¸²à¸ªà¸¹à¹ˆ RAG pipeline... à¸à¸³à¸¥à¸±à¸‡à¹€à¸£à¸µà¸¢à¸ retriever à¹€à¸žà¸·à¹ˆà¸­à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥...")
                
                # We can remove the intermediate debugging steps if the core functionality works.
                # These were useful for initial debugging but might clutter the UI in production.
                # st.write("3. à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ˆà¸­à¹à¸¥à¹‰à¸§ ... à¸à¸³à¸¥à¸±à¸‡à¸ˆà¸°à¸ªà¹ˆà¸‡à¹„à¸›à¹ƒà¸«à¹‰ LLM...")

                # à¸£à¸±à¸™ pipeline à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
                response = rag_pipeline({"query": prompt})
                
                st.write("4. à¹„à¸”à¹‰à¸£à¸±à¸šà¸„à¸³à¸•à¸­à¸šà¸ˆà¸²à¸ LLM à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢à¹à¸¥à¹‰à¸§! à¸à¸³à¸¥à¸±à¸‡à¸ˆà¸°à¹à¸ªà¸”à¸‡à¸œà¸¥...")

                full_response = response['result']
                
                # à¹à¸ªà¸”à¸‡à¹à¸«à¸¥à¹ˆà¸‡à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡
                source_docs = response['source_documents']
                if source_docs:
                    full_response += "\n\n---\n**à¹à¸«à¸¥à¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡:**\n"
                    unique_sources = set([doc.metadata['source'] for doc in source_docs if 'source' in doc.metadata]) # Added check for 'source' key
                    for source in unique_sources:
                        full_response += f"- {os.path.basename(source)}\n"
                
                st.markdown(full_response)
        
        except Exception as e:
            st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¸£à¹‰à¸²à¸¢à¹à¸£à¸‡à¸‚à¸¶à¹‰à¸™: {e}")
            full_response = "à¸‚à¸­à¸­à¸ à¸±à¸¢ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸„à¸³à¸–à¸²à¸¡à¸‚à¸­à¸‡à¸„à¸¸à¸“ à¹‚à¸›à¸£à¸”à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡à¹ƒà¸™à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡" # Provide a fallback message

        st.write("5. à¸ˆà¸šà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™à¹ƒà¸™à¸ªà¹ˆà¸§à¸™ assistant")
        # --- DEBUGGING ENDS HERE ---
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})